Partner Bootcamp: Evals Starter Lab
Audio Evals with OpenAI

Need an audio-first walkthrough? Jump to the new Audio Evals lab for a step-by-step guide.
Time investment: expect ~45 minutes of hands-on building to complete the challenge.

Facilitated by OpenAI Technical Success: delivered directly by the internal TS team—not a generic training vendor.

Fresh enterprise signal: curriculum reflects the latest H2 2025 enterprise Evals deployments we’ve been running.

Lab type

Hands-on challenge

Duration

~45 minutes

Level

Advanced builders

Environment

macOS/Linux/Windows terminal

Python

3.10+

Jump to

Intro & environment setup
Schema & testing criteria
Create evals & runs
Analyze output items
Bulk prompt/model experiments
CI regression guardrails
1. Intro & environment setup
Evaluations (evals) help you build high-quality LLM apps by checking whether model outputs meet accuracy criteria you define. Think of them as unit tests for prompts, models, and pipelines. The Evaluations API lets you create and run evals directly from your dev environment so you can measure new prompts, compare experiments, or validate CI changes with confidence.

2. Schema & testing criteria
Import the respective packages:

%pip install openai --quiet

import os, json
from openai import OpenAI
Every long-lived eval starts with a stable schema. Define the shape of your item (input data) and optionally a sample schema (model response) before running any experiments.

data = [
  '- New message from Sarah: "Can you call me later?"- Your package has been delivered!- Flash sale: 20% off electronics for the next 2 hours!',
  "- Weather alert: Thunderstorm expected in your area.- Reminder: Doctor's appointment at 3 PM.- John liked your photo on Instagram.",
  ...
]

file_content = [{"item": {"push_notifications": x}} for x in data]

data_source_config = {
  "type": "custom",
  "item_schema": {
      "type": "object",
      "properties": {"push_notifications": {"type": "string"}},
      "required": ["push_notifications"],
  },
  "include_sample_schema": True,
}
With this config you can reference {{item.push_notifications}} and {{sample.output_text}} inside graders.

Next, define test criteria. Each entry in testing_criteria targets one aspect of quality. Choose from label_model, string_check, or text_similarity.

testing_criteria = [
  {
      "type": "label_model",
      "model": "gpt-4.1",
      "input": [
          {"role": "developer", "content": "Label the following push notification summary as correct if it reflects the list accurately; otherwise incorrect."},
          {
              "role": "user",
              "content": (
                  "<list_of_notifications>{{item.push_notifications}}</list_of_notifications>"
                  "<summary>{{sample.output_text}}</summary>"
              ),
          },
      ],
      "passing_labels": ["correct"],
      "labels": ["correct", "incorrect"],
      "name": "Push Notification Summary Accuracy",
  }
]
3. Create evals & kick off runs
Create the Eval once, then attach runs that score model outputs. You can generate completions on the fly or upload JSONL with pre-computed results.

eval_object = client.evals.create(
  name="Push Notification Summary Workflow",
  data_source_config=data_source_config,
  testing_criteria=testing_criteria,
)
print(f"Created eval: {eval_object.id}")
To generate and score completions immediately:

sampling_messages = [
  {
      "role": "system",
      "content": (
          "You are a helpful assistant that takes in an array of push notifications and returns a concise summary."
      ),
  },
  {
      "role": "user",
      "content": [
          {
              "type": "input_text",
              "text": "Summarize these notifications in one short sentence: {{ item.push_notifications }}",
          },
      ],
  },
]

from openai import OpenAI
client = OpenAI()

run = client.evals.runs.create(
  eval_id=eval_object.id,
  name="Summarization Eval Run",
  data_source={
      "type": "responses",
      "model": "gpt-5.1",
      "input_messages": {
          "type": "template",
          "template": [
              {"role": "developer", "content": "You are an expert in summarizing push notifications. Given the push notifications below, summarize the request into one short sentence."},
              {"role": "user", "content": "{{ item.push_notifications }}"},
          ],
      },
      "source": {"type": "file_content", "content": file_content},
  },
)

print(run)
To evaluate stored outputs (JSONL) or external model results, provide type: "jsonl" and include both item and sample records.

4. Analyze output items
Use the report_url returned from each run to inspect pass/fail summaries and drill into output items.

Output Item = input data + model/sample response + grader verdicts.
Aggregate pass/fail across items to quantify regressions.
Inspect numeric score fields and conversation transcripts for deeper debugging.
Use datasource_item_id to correlate failures back to original prompts.
5. Bulk prompt/model experiments
Test multiple prompt templates against a set of models to see which combination meets your criteria.

PROMPT_PREFIX = """
You are a helpful assistant that takes in an array of push notifications and returns a collapsed summary of them.
"""
PROMPT_VARIATION_BASIC = f"""
{PROMPT_PREFIX}
You should return a summary that is concise and snappy.
"""
PROMPT_VARIATION_WITH_EXAMPLES = f"""
{PROMPT_VARIATION_BASIC}

Here is an example of a good summary:
&lt;push_notifications&gt;...&lt;/push_notifications&gt;
&lt;summary&gt;Traffic alert, package expected by 5pm, suggestion for new friend (Emily).&lt;/summary&gt;
"""
... # include negative example variation
prompts = [("basic", PROMPT_VARIATION_BASIC), ...]
models = ["gpt-4.1", "gpt-4.1", "gpt-5"]
for prompt_name, prompt in prompts:
  for model in models:
      run_data_source = {
          "type": "completions",
          "input": [
              {"role": "system", "content": prompt},
              {
                  "role": "user",
                  "content": "&lt;push_notifications&gt;{{item.push_notifications}}&lt;/push_notifications&gt;",
              },
          ],
          "model": model,
          "source": {
              "type": "file_content",
              "content": [{"item": item} for item in item_data],
          },
      }
      requests.post(
          f"https://api.openai.com/v1/evals/{eval_id}/runs",
          headers={"Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"},
          json={"name": f"bulk_{prompt_name}_{model}", "data_source": run_data_source},
      )
Wrap the snippets above in helper scripts (setup_eval, add_bulk_runs) so facilitators can reuse them.

6. CI regression guardrails
Use eval runs inside CI to detect prompt or code regressions before merging.

Simulate passing vs failing prompt versions and attach them to PR checks:

def summarize_push_notification_head(push_notifications: str) -> ChatCompletion:
  return openai.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": "...concise and snappy..."},
          {"role": "user", "content": push_notifications},
      ],
  )

def summarize_push_notification_pr(push_notifications: str) -> ChatCompletion:
  return openai.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": "...long-winded..."},
          {"role": "user", "content": push_notifications},
      ],
  )
if __name__ == "__main__":
  if sys.argv[1] == "setup-eval":
      setup_eval()
  elif sys.argv[1] == "add-eval-run" and sys.argv[2] in {"head", "pr"}:
      add_eval_run(sys.argv[2], sys.argv[3])
Call setup-eval, then add runs for head and pr branches to catch regressions automatically.


### Advanced RAG Lab: Multi-Tool Orchestration with Responses API + Pinecone + Web Search
What you’ll build

An end-to-end, agentic RAG system that routes queries to either Pinecone (for domain knowledge) or the built-in web-search tool using OpenAI’s Responses API. You will embed a medical reasoning dataset, index it in Pinecone, and orchestrate multiple tools so the model auto-selects the right source.
Table of Contents
Objectives
Prerequisites
Architecture
Setup
Step 1 – Load & prep dataset
Step 2 – Create Pinecone index
Step 3 – Upsert embeddings
Step 4 – Query helper
Step 5 – Context → Responses API
Step 6 – Tool orchestration
Next steps
Objectives
Embed a domain-specific dataset and store it in Pinecone.
Build semantic + keyword retrieval helpers.
Use Responses API to synthesize grounded answers with citations.
Orchestrate tools (web search + Pinecone) so the model chooses intelligently.
Prerequisites
Python 3.10+
OpenAI API key (OPENAI_API_KEY)
Pinecone API key (PINECONE_API_KEY)
Optional: Hugging Face access (for the sample dataset)
Packages: openai, pinecone-client, datasets, pandas, tqdm
Install dependencies:
pip install openai pinecone-client datasets pandas tqdm
Export keys:
export OPENAI_API_KEY="your-openai-key"
export PINECONE_API_KEY="your-pinecone-key"
Architecture
User Query
   │
   ├─► Responses API (tool orchestration)
   │       │
   │       ├─► web_search_preview (for general / fresh facts)
   │       └─► PineconeSearchDocuments (semantic medical retrieval)
   │
   └─► Grounded answer with citations (doc ids / titles)
Setup
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install openai pinecone-client datasets pandas tqdm
Step 1 – Load & prep dataset
We’ll use a medical reasoning dataset and merge question + response into one field for embedding.
from datasets import load_dataset
from pandas import DataFrame

ds = load_dataset(
    "FreedomIntelligence/medical-o1-reasoning-SFT",
    "en",
    split="train[:100]",
    trust_remote_code=True,
)
df = DataFrame(ds)

df["merged"] = df.apply(
    lambda row: f"Question: {row['Question']} Answer: {row['Response']}",
    axis=1,
)
print("Example merged text:", df["merged"].iloc[0])
Step 2 – Create Pinecone index
Compute embedding dimension, create a serverless index.
import os, time, random, string
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL = "text-embedding-3-small"

sample = client.embeddings.create(input=[df["merged"].iloc[0]], model=MODEL)
embed_dim = len(sample.data[0].embedding)
print("Embedding dimension:", embed_dim)

pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
spec = ServerlessSpec(cloud="aws", region="us-east-1")
index_name = "pinecone-index-" + "".join(
    random.choices(string.ascii_lowercase + string.digits, k=10)
)
if index_name not in pc.list_indexes().names():
    pc.create_index(index_name, dimension=embed_dim, metric="dotproduct", spec=spec)
index = pc.Index(index_name)
time.sleep(1)
print("Index ready")
Step 3 – Upsert embeddings
Batch embed and upsert with metadata.
from tqdm.auto import tqdm

batch_size = 32
for i in tqdm(range(0, len(df["merged"]), batch_size), desc="Upserting to Pinecone"):
    i_end = min(i + batch_size, len(df["merged"]))
    lines_batch = df["merged"][i:i_end]
    ids_batch = [str(n) for n in range(i, i_end)]

    res = client.embeddings.create(input=list(lines_batch), model=MODEL)
    embeds = [record.embedding for record in res.data]

    metadata = []
    for record in df.iloc[i:i_end].to_dict("records"):
        metadata.append({"Question": record["Question"], "Answer": record["Response"]})

    vectors = list(zip(ids_batch, embeds, metadata))
    index.upsert(vectors=vectors)
Step 4 – Query helper
Embed the query and fetch top-k matches.
def query_pinecone_index(client, index, model, query_text, top_k=5):
    q_emb = client.embeddings.create(input=query_text, model=model).data[0].embedding
    res = index.query(vector=[q_emb], top_k=top_k, include_metadata=True)
    print("Query Results:")
    for match in res["matches"]:
        print(f"{match['score']:.2f}: {match['metadata'].get('Question','')} -> "
              f"{match['metadata'].get('Answer','')}")
    return res

test_query = (
    "A 45-year-old man with confusion, ataxia, and ophthalmoplegia. "
    "What is the likely diagnosis and recommended treatment?"
)
query_pinecone_index(client, index, MODEL, test_query)
Step 5 – Context → Responses API
Take top matches, build context, and ask the model to answer with citations. Update the model configuration to test the model output with varying parameters.
matches = index.query(
    vector=[client.embeddings.create(input=test_query, model=MODEL).data[0].embedding],
    top_k=3,
    include_metadata=True,
)["matches"]

context = "\n\n".join(
    f"Question: {m['metadata'].get('Question','')}\n"
    f"Answer: {m['metadata'].get('Answer','')}"
    for m in matches
)

resp = client.responses.create(
    model="gpt-5.1",
    input=f"Provide the answer based on the context: {context}\nQuestion: {test_query}",
)
print("Final Answer:", resp.output_text)
Step 6 – Tool orchestration
Define tools: built-in web search and custom Pinecone search. The model chooses the right tool per query.
tools = [
    {
        "type": "web_search_preview",
        "user_location": {"type": "approximate", "country": "US", "region": "California", "city": "San Francisco"},
        "search_context_size": "medium",
    },
    {
        "type": "function",
        "name": "PineconeSearchDocuments",
        "description": "Semantic medical search over the Pinecone index.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "User query to search vector DB."},
                "top_k": {"type": "integer", "description": "Top results to return.", "default": 3},
            },
            "required": ["query"],
            "additionalProperties": False,
        },
    },
]
Driver loop: run queries, execute tool calls, and return final answers.
queries = [
    {"query": "Who won the cricket world cup in 1983?"},
    {"query": "What is the most common cause of death in the United States?"},
    {"query": (
        "A 7-year-old with sickle cell disease has hip pain, limp, and pain with ambulation. "
        "What is the next step in management?"
    )},
]

for item in queries:
    print("\n--- Processing Query ---")
    print("User Query:", item["query"])
    input_messages = [
        {"role": "system", "content": "Always use a tool. Use web_search_preview for non-medical or fresh questions. Use PineconeSearchDocuments only for medical/clinical questions. Do not answer directly."},
        {"role": "user", "content": item["query"]},
    ]

    response = client.responses.create(
        model="gpt-5.1",
        input=input_messages,
        tools=tools,
        parallel_tool_calls=True,
        tool_choice="required",
    )

    tool_call = next(
        (output for output in response.output if output.type in ("function_call", "web_search_call")),
        None,
    )

    if tool_call is None:
        print("Model triggered: none")
        print("Final Answer:", response.output_text)
        continue

    if tool_call.type == "web_search_call":
        print("Model triggered: web_search_preview")
        print("Final Answer:", response.output_text)
        continue

    tool_name = tool_call.name
    print("Tool triggered:", tool_name)

    if tool_name == "PineconeSearchDocuments":
        res = query_pinecone_index(client, index, MODEL, item["query"])
        if res["matches"]:
            best = res["matches"][0]["metadata"]
            result = f"Question: {best.get('Question','')}\nAnswer: {best.get('Answer','')}"
        else:
            result = "No matching documents found."
    else:
        result = "Simulated web search result (replace with real search)."

    input_messages.append(tool_call)
    input_messages.append({
        "type": "function_call_output",
        "call_id": tool_call.call_id,
        "output": result,
    })

    final_response = client.responses.create(
        model="gpt-5.1",
        input=input_messages,
        tools=tools,
        parallel_tool_calls=True,
    )
    print("Final Answer:", final_response.output_text)
Run the driver loop and confirm the routing and final responses.
Expected behavior:
General questions → web search.
Medical reasoning → Pinecone search.
Next steps
Label a small set of queries with the expected tool, run the router, and compute accuracy + mismatches. This makes it obvious when the model over‑routes to Pinecone.
Attach logging/metrics to evaluate tool choices and answer quality.
Add web based domain filtering to only select information from relevant docs.
Add reranking or weighting between web search and vector search.
Add guardrails step to ensure right safety nets are in place.
Swap in your own domain docs and persist embeddings to a dedicated Pinecone project.
Deploy as an API (FastAPI) or chatbot UI; cache frequent queries for cost savings.
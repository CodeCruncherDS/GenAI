Realtime Voice Agent Quickstart
Build a speech-to-speech agent with the Realtime API

Spin up a Python voice agent that listens continuously, streams your speech to the OpenAI Realtime API, and answers back with natural audio.
Lab type

Guided build

Duration

~45 minutes

Level

Intermediate

Environment

Local terminal + mic

Python

3.9+

Focus

Realtime voice agents

Jump to

Overview
Prerequisites
Environment setup
Build the agent
Config options
Next steps
Practice
1. Overview of realtime voice agents
The Realtime API lets agents converse through speech-to-speech. Rather than waiting for you to finish talking, the model can start responding mid-utterance. The pipeline runs in three phases:

Speech-to-text converts microphone audio to text.
Your code / agent workflow decides what to do, including tool calls.
Text-to-speech turns the response back into audio.
When you connect these stages, you get natural, interruptible conversations. This lab uses the Python SDK for clarity and local experimentation.

2. Prerequisites
Python 3.9+ and a terminal.
An OpenAI API key with Realtime access.
Basic familiarity with the Agents SDK.
A microphone and speakers/headphones.
Realtime agents are in beta, so expect occasional breaking changes.

3. Environment setup
Create a project folder
mkdir realtime-voice-agent
cd realtime-voice-agent
Create a virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
Install the OpenAI Agents SDK with voice extras
pip install "openai-agents[voice]"
Set your OpenAI API key (do not hard-code it in scripts)
export OPENAI_API_KEY="sk-...your-key-here"
Optional: generate an ephemeral client token for browser apps by POSTing to https://api.openai.com/v1/realtime/client_secrets. This lab runs locally, so an API key is enough.
4. Build a realtime voice agent in Python
4.1 Import required components
import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner
import sounddevice as sd
Install sounddevice with pip install sounddevice. Some platforms need system audio libs (for example, portaudio).

4.2 Create a realtime agent
assistant = RealtimeAgent(
  name="Assistant",
  instructions=(
      "You are a helpful voice assistant. "
      "Keep responses brief and conversational."
  ),
)
Instructions shape the model's behavior. Keep them concise and voice-friendly.

4.3 Configure the runner
runner = RealtimeRunner(
  starting_agent=assistant,
  config={
      "model_settings": {
          "model_name": "gpt-realtime",
          "voice": "ash",
          "modalities": ["audio"],
          "input_audio_format": "pcm16",
          "output_audio_format": "pcm16",
          "input_audio_transcription": {
              "model": "gpt-4o-mini-transcribe"
          },
          "turn_detection": {
              "type": "semantic_vad",
              "interrupt_response": True
          }
      }
  }
)
model_name: try gpt-realtime.
voice: choose from voices like ash, marin, or cedar.
turn_detection: semantic_vad auto-detects pauses; interrupt_response=True allows you to speak over the agent.
4.4 Start a session and process events
Use an async context manager to handle connection lifetime and stream events.

import asyncio
from agents.realtime import RealtimeAgent, RealtimeRunner
import sounddevice as sd

async def main():
  agent = RealtimeAgent(
      name="Assistant",
      instructions="You are a helpful voice assistant. Keep responses brief and conversational.",
  )

  runner = RealtimeRunner(
      starting_agent=agent,
      config={
          "model_settings": {
              "model_name": "gpt-realtime",
              "voice": "ash",
              "modalities": ["audio"],
              "input_audio_format": "pcm16",
              "output_audio_format": "pcm16",
              "input_audio_transcription": {"model": "gpt-4o-mini-transcribe"},
              "turn_detection": {"type": "semantic_vad", "interrupt_response": True},
          }
      },
  )

  session = await runner.run()

  audio_queue: asyncio.Queue[bytes] = asyncio.Queue()

  def audio_callback(outdata, frames, time, status):
      try:
          data = audio_queue.get_nowait()
      except asyncio.QueueEmpty:
          outdata[:] = b"" * len(outdata)
          return
      outdata[:] = data

  async with session:
      print("Session started! Speak into your microphone.")
      with sd.OutputStream(
          samplerate=16000, channels=1, dtype="int16", callback=audio_callback
      ):
          async for event in session:
              try:
                  if event.type == "agent_start":
                      print(f"Agent started: {event.agent.name}")
                  elif event.type == "agent_end":
                      print(f"Agent ended: {event.agent.name}")
                  elif event.type == "tool_start":
                      print(f"Tool started: {event.tool.name}")
                  elif event.type == "tool_end":
                      print(f"Tool ended: {event.tool.name}; output: {event.output}")
                  elif event.type == "audio":
                      await audio_queue.put(event.data)
                  elif event.type == "audio_end":
                      print("Audio ended")
                  elif event.type == "audio_interrupted":
                      print("Audio interrupted - flushing buffer")
                  elif event.type == "error":
                      print(f"Error: {event.error}")
              except Exception as e:
                  print(f"Error processing event: {e}")

if __name__ == "__main__":
  asyncio.run(main())
Session creation: runner.run() yields a RealtimeSession that streams events.
Audio output: sounddevice.OutputStream plays 16-bit PCM audio at 16 kHz.
Event loop: handle audio, agent_start, tool_start, and error events as they arrive.
5. Configuration options
Common knobs from the quickstart:

Parameter	Purpose
model_name	Realtime model name (for example, gpt-realtime).
voice	Voice style for TTS (for example, ash, echo).
modalities	Enabled modalities (typically ["audio"]).
input_audio_format	Incoming audio format (for example, pcm16).
output_audio_format	Format for synthesized speech.
input_audio_transcription	Speech-to-text model (for example, {"model": "gpt-4o-mini-transcribe"}).
turn_detection	Detection mode (for example, semantic_vad) and whether to allow interruptions.
These settings mirror the voice pipeline (speech-to-text, agent processing, text-to-speech) so you can tune latency and quality.

6. Next steps and extensions
Add tool calling: register functions (weather, device control) and handle tool_start/tool_end.
Multi-agent handoffs: create specialist agents and hand off by intent.
Transcription logging: record user/agent transcripts with timestamps.
Custom audio devices: swap sounddevice for pyaudio or soundcard as needed.
Browser-based clients: generate client secrets and connect via the TypeScript SDK.
Guardrails and context: constrain outputs and manage conversation state across turns.
7. Practice challenges
Change the voice: switch to another voice and compare tone.
Add a custom tool: expose get_current_time and let the agent answer "What time is it?".
Implement memory: use Memory to remember user facts across turns.
Voice interruption: toggle interrupt_response to experience full vs. interruptible speech.
Browser demo: build a small web front end that connects via WebSocket using an ephemeral key.
Multi-agent scenario: route between two specialists (for example, travel and weather).
Error handling: intentionally call a missing tool and handle error events gracefully.
MCP-Powered Voice Support Assistant
Build a voice-first, tool-using agent

Create a customer-support voice assistant that routes across RAG, SQLite, and web search tools via the Model Context Protocol (MCP). This tutorial mirrors the architecture of the MCP voice framework, adapted for a builder-lab setting.
Lab type

Guided build

Duration

~60 minutes

Level

Intermediate

Environment

macOS/Linux/Windows terminal

Python

3.10+

Focus

MCP + Voice Agents

Jump to

What is MCP
High-level flow
Environment setup
Custom MCP tool server
SQLite tool
Insurance agent
Voice config
Real-time audio loop
Orchestrating servers
Run the assistant
Traces & latency
Conclusion
1. What is MCP and why use it?
Model Context Protocol (MCP) standardizes how models connect to tools and data sources, decoupling agent logic from tool implementations.
It enforces consistent security/governance and lets you swap or update tools without changing agent code.
Think of MCP as the ‚ÄúUSB-C port‚Äù for AI applications‚Äîplug in retrieval, databases, or search with a common interface.
2. High-level flow
Capture speech and transcribe to text.
Insurance agent chooses a tool: RAG for FAQs/policies, SQLite for product lookups, or web search for general questions.
Tool executes; insurance synthesizes a concise, friendly response.
Convert response to audio via TTS and play it back.
3. Environment setup
Install Python deps and voice extras:

pip install asyncio ffmpeg ffprobe mcp openai openai-agents pydub scipy sounddevice uv --quiet
pip install "openai-agents[voice]" --quiet
Ensure ffmpeg and ffprobe are installed (e.g., brew install ffmpeg). Set OPENAI_API_KEY in your environment.

4. Custom MCP tool service
Host RAG + web search tools in a FastMCP server (tool_server.py):

import os
from mcp.server.fastmcp import FastMCP
from openai import OpenAI
from agents import set_tracing_export_api_key

mcp = FastMCP("SupportTools")
_vector_store_id = ""

def _run_rag(query: str) -> str:
  results = client.vector_stores.search(
      vector_store_id=_vector_store_id,
      query=query,
      rewrite_query=True,
  )
  return results.data[0].content[0].text

def _summarize_rag_output(rag_output: str) -> str:
  response = client.responses.create(
      model="gpt-4.1-mini",
      tools=[{"type": "web_search_preview"}],
      input="Summarize the following text concisely:\n\n" + rag_output,
  )
  return response.output_text

@mcp.tool()
def retrieve_faq(query: str) -> str:
  rag_output = _run_rag(query)
  return _summarize_rag_output(rag_output)

@mcp.tool()
def web_lookup(query: str) -> str:
  response = client.responses.create(
      model="gpt-4.1-mini",
      tools=[{"type": "web_search_preview"}],
      input=query,
  )
  return response.output_text

def index_documents(directory: str):
  SUPPORTED_EXTENSIONS = {'.pdf', '.txt', '.md', '.docx', '.pptx', '.csv', '.rtf', '.html', '.json', '.xml'}
  files = [os.path.join(directory, f) for f in os.listdir(directory)]
  supported_files = [f for f in files if os.path.splitext(f)[1].lower() in SUPPORTED_EXTENSIONS]
  vector_store = client.vector_stores.create(name="SupportFAQ")
  global _vector_store_id
  _vector_store_id = vector_store.id
  for file_path in supported_files:
      with open(file_path, "rb") as fp:
          client.vector_stores.files.upload_and_poll(vector_store_id=vector_store.id, file=fp)

if __name__ == "__main__":
  oai_api_key = os.environ.get("OPENAI_API_KEY")
  if not oai_api_key:
      raise ValueError("OPENAI_API_KEY environment variable is not set")
  set_tracing_export_api_key(oai_api_key)
  client = OpenAI(api_key=oai_api_key)

  current_dir = os.path.dirname(os.path.abspath(__file__))
  samples_dir = os.path.join(current_dir, "sample_files")
  index_documents(samples_dir)
  mcp.run(transport="sse")
Creates a FastMCP server named SupportTools.
Exposes retrieve_faq (RAG) and web_lookup (web search).
Indexes files from sample_files into a vector store on startup.
Once the above tool_server.py file is executed, you can navigate to your vector store and find the Evergreen_Health_Platinum_Plan.pdf created in your platform.
vector_store
5. Use the SQLite MCP tool
Re-use the standard SQLite MCP server for product data lookups. Ensure database.db exists with a products table; the main script will launch the server via mcp-server-sqlite.

6. Build the insurance agent
Voice-friendly agent setup (main.py):

async def create_insurance_agents(mcp_servers: list[MCPServer]) -> Agent:
  """Create the insurance agent workflow with voice optimization"""
  
  # Main insurance agent with MCP tools
  insurance_agent = Agent(
      name="InsuranceAssistant",
      instructions=voice_system_prompt + prompt_with_handoff_instructions("""
          #Identity
          You an a helpful chatbot that answers questions about our insurance plans. 
          #Task
          Use the tools provided to answer the questions. 
          #Instructions
          * Information about plans and policies are best answered with sqlite or rag_output tools.
          * web_search should be used for answering generic health questions that are not directly related to our insurance plans.
          * Always respond in English, even if the user asks in another language.
          * If your confidence is low, try use another tool.
      """),
      mcp_servers=mcp_servers,
      model="gpt-4.1",
      )
  
  return insurance_agent
  
7. Configure voice
Voice pipeline settings (voice_config.py):

async def continuous_voice_conversation(agent: Agent):
  """User-friendly voice conversation: Press Enter to start and stop recording, then process speech."""
  voice_config = VoicePipelineConfig(
      tts_settings=insurance_tts_settings,
  )
  pipeline = VoicePipeline(
      workflow=SingleAgentVoiceWorkflow(agent),
      config=voice_config
  )
  print("üéôÔ∏è Insurance Voice Assistant Ready!")
  print("Press [Enter] to START recording, [Enter] again to STOP and process. Type 'esc' to exit.")
  print("-" * 50)
  import scipy.io.wavfile
  import time
  while True:
      cmd = input("Press Enter to start recording (or type 'esc' to exit): ")
      if cmd.strip().lower() == "esc":
          print("Exiting... Goodbye!")
          break
      print("Listening... Press Enter again to stop recording.")
      recorded_chunks = []
      recording = True
      # Start audio input stream
      def input_callback(indata, frames, time_info, status):
          if status:
              print(f"Audio input status: {status}")
          recorded_chunks.append(indata.copy())
      stream = sd.InputStream(
          samplerate=AUDIO_CONFIG["samplerate"],
          channels=AUDIO_CONFIG["channels"],
          dtype=AUDIO_CONFIG["dtype"],
          callback=input_callback,
          blocksize=AUDIO_CONFIG["blocksize"],
          device=AUDIO_CONFIG["input_device"]
      )
      stream.start()
      input("[Recording...] Press Enter to stop: ")
      stream.stop()
      stream.close()
      if not recorded_chunks:
          print("No audio captured. Try again.")
          continue
      full_audio = np.concatenate(recorded_chunks, axis=0)
      if len(full_audio) < AUDIO_CONFIG["samplerate"] * AUDIO_CONFIG["min_speech_duration"]:
          print("Recording too brief. Try again.")
          continue
      print("
ü§î Processing speech...")
      # Save WAV temporary file for OpenAI STT
      with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
          scipy.io.wavfile.write(tmp_wav.name, AUDIO_CONFIG["samplerate"], full_audio)
          tmp_wav.flush()
          tmp_wav_path = tmp_wav.name
      transcript = ""
      try:
          openai.api_key = os.environ.get("OPENAI_API_KEY")
          with open(tmp_wav_path, "rb") as audio_file:
              stt_response = openai.audio.transcriptions.create(
                  model="gpt-4o-transcribe",
                  file=audio_file,
                  response_format="text"
              )
              transcript = stt_response.strip()
      except Exception as stt_err:
          print(f"[STT Error]: {stt_err}")
      finally:
          os.remove(tmp_wav_path)
      print(f"[STT Transcript]: {transcript}")
      if not transcript:
          print("No speech detected, try again...")
          continue
      audio_input = AudioInput(buffer=full_audio)
      with trace("Insurance Voice Query"):
          result = await pipeline.run(audio_input)
          print("üí¨ Assistant responding...")
          streamed_audio_chunks = []
          all_transcript = ''
          async for event in result.stream():
              if event.type == "voice_stream_event_audio":
                  streamed_audio_chunks.append(event.data)
              elif event.type == "voice_stream_event_transcript":
                  print(f"   > {event.text}", end="", flush=True)
                  all_transcript += event.text
          # Play full audio response if present
          if streamed_audio_chunks:
              response_audio = np.concatenate(streamed_audio_chunks, axis=0)
              sd.play(response_audio, samplerate=AUDIO_CONFIG["samplerate"])
              sd.wait()
          print("
")
          # Detect what tool was called by scanning the transcript
          tool_name = None
          lower_transcript = all_transcript.lower()
          if "rag" in lower_transcript:
              tool_name = "rag_output"
          elif "sqlite" in lower_transcript or "database" in lower_transcript:
              tool_name = "sqlite"
          elif "web_search" in lower_transcript or "web search" in lower_transcript:
              tool_name = "web_search"
          if tool_name:
              print(f"[Agent Handoff]: Tool called - {tool_name}")

class AudioStreamManager:
  """Context manager for handling audio streams"""
  def __init__(self, input_stream: sd.InputStream, output_stream: sd.OutputStream):
      self.input_stream = input_stream
      self.output_stream = output_stream

  async def __aenter__(self):
      try:
          self.input_stream.start()
          self.output_stream.start()
          return self
      except sd.PortAudioError as e:
          raise RuntimeError(f"Failed to start audio streams: {e}")

  async def __aexit__(self, exc_type, exc_val, exc_tb):
      try:
          if self.input_stream:
              self.input_stream.stop()
              self.input_stream.close()
          if self.output_stream:
              self.output_stream.stop()
              self.output_stream.close()
      except Exception as e:
          print(f"Warning: Error during audio stream cleanup: {e}")

def print_audio_devices():
  print("
Available audio devices:")
  for idx, dev in enumerate(sd.query_devices()):
      print(f"{idx}: {dev['name']} - Input Channels: {dev['max_input_channels']} | Output Channels: {dev['max_output_channels']}")
  print("")

print_audio_devices()  # Call once on launch
9. Orchestrate servers and agent
Main entrypoint (main.py): launch the custom SSE server, the SQLite server, build the agent, and start the voice loop.

import os
import nest_asyncio
import asyncio
import shutil
import subprocess
from agents.mcp import MCPServerSse, MCPServerStdio
from agent_setup import create_support_agent
from voice_chat import continuous_voice_chat
from tool_server import wait_for_server_ready  # helper to poll readiness

class ServerProcess:
  def __init__(self, server_file: str):
      self.server_file = server_file
      self.process: subprocess.Popen | None = None

  async def __aenter__(self):
      if not shutil.which("uv"):
          raise RuntimeError("uv is not installed. Install it from https://docs.astral.sh/uv/getting-started/installation/")
      print("Starting custom SSE server on http://localhost:8000/sse ‚Ä¶")
      self.process = subprocess.Popen(["uv", "run", self.server_file])
      try:
          await wait_for_server_ready()
          nest_asyncio.apply()
          return self
      except Exception as e:
          if self.process:
              self.process.terminate()
          raise RuntimeError(f"Failed to start SSE server: {e}")

  async def __aexit__(self, exc_type, exc_val, exc_tb):
      if self.process:
          self.process.terminate()
          self.process.wait(timeout=5)

async def main():
  this_dir = os.getcwd()
  server_file = os.path.join(this_dir, "tool_server.py")

  async with ServerProcess(server_file):
      async with MCPServerSse(
          name="SupportToolSSE",
          params={"url": "http://localhost:8000/sse", "timeout": 15.0},
          client_session_timeout_seconds=15.0,
      ) as support_server:
          async with MCPServerStdio(
              cache_tools_list=True,
              params={"command": "uvx", "args": ["mcp-server-sqlite", "--db-path", "./database.db"]},
          ) as sqlite_server:
              agent = await create_support_agent([support_server, sqlite_server])
              await continuous_voice_chat(agent)

if __name__ == "__main__":
  asyncio.run(main())
10. Run the assistant
Place FAQ docs in ./support_files (PDF/MD/TXT).
Create database.db with product tables.
Export OPENAI_API_KEY in your shell.
Run python tool_server.py (optional) then python main.py to start both servers and the voice assistant.
Example prompts: ‚ÄúHow do I reset my password?‚Äù (RAG), ‚ÄúWhich plans cost under $100?‚Äù (SQLite), ‚ÄúWhat is two-factor authentication?‚Äù (web search).

11. Monitor traces and latency
The Agents SDK records model + tool calls in the Traces dashboard. Use it to inspect inputs/outputs and diagnose latency spikes (e.g., slow web lookups).

12. Added challenge: price comparison tool
Extend the tutorial with a focused, doable upgrade:

Create a new MCP tool (e.g., compare_prices(product_name: str) -> str) that pulls latest prices from an external API or a mock JSON file. Decorate it with @mcp.tool() in tool_server.py.
Extend SQLite with a products table (name, manufacturer, base_price) so the agent can cross-reference internal data with external prices.
Update agent instructions to call compare_prices when users ask for cheapest/best price comparisons.
Test with voice: ask ‚ÄúWhich vendor has the lowest price for Product X?‚Äù or ‚ÄúCompare Product Y prices between us and other vendors.‚Äù
Inspect traces to confirm the tool is invoked and to review latency/end-to-end orchestration.
This challenge exercises MCP tool creation, agent prompt design, and voice orchestration while adding real utility.

13. Conclusion
This lab walks through building a voice-driven customer-support agent using MCP and the Agents SDK: custom FastMCP tools for RAG and web search, the SQLite MCP server for product data, a planner tuned for voice, and a real-time STT/TTS loop. Customize prompts, models, and tools to fit your own builder-lab experiments.